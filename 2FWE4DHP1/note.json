{
  "paragraphs": [
    {
      "text": "%md\n# About This Lab\n**Objective:** In this lab, you will use Zeppelin to work with DataFrames.\nBegin by learning about Apache Spark documentation and JavaDocs. Read and\ndisplay a JSON file into a DataFrame. Query the DataFrame.\n**File locations:** \n    Exercises directory: /home/training/training_materials/devsh/exercises/spark-shell\n    Data (local): /home/training/training_materials/devsh/data/devices.json\n**Successful outcome:**\n**Before you begin:**\n**Related lessons:** Apache Spark Basics\n\n---",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.518",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch1\u003eAbout This Lab\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eObjective:\u003c/strong\u003e In this lab, you will use Zeppelin to work with DataFrames.\n\u003cbr  /\u003eBegin by learning about Apache Spark documentation and JavaDocs. Read and\n\u003cbr  /\u003edisplay a JSON file into a DataFrame. Query the DataFrame.\n\u003cbr  /\u003e\u003cstrong\u003eFile locations:\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eExercises directory: /home/training/training_materials/devsh/exercises/spark-shell\nData (local): /home/training/training_materials/devsh/data/devices.json\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSuccessful outcome:\u003c/strong\u003e\n\u003cbr  /\u003e\u003cstrong\u003eBefore you begin:\u003c/strong\u003e\n\u003cbr  /\u003e\u003cstrong\u003eRelated lessons:\u003c/strong\u003e Apache Spark Basics\u003c/p\u003e\n\u003chr /\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727515_2044962370",
      "id": "20171105-200834_1116095891",
      "dateCreated": "2021-01-30 19:58:47.515",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n# Setup\n---",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.522",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch1\u003eSetup\u003c/h1\u003e\n\u003chr /\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727522_1621381343",
      "id": "20181114-164229_902436001",
      "dateCreated": "2021-01-30 19:58:47.522",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Environment variable required to use SetJobGroup",
      "text": "%sh\n\nPYSPARK_PIN_THREAD\u003dtrue",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.522",
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1612036727522_2076707385",
      "id": "20200830-062347_1042316978",
      "dateCreated": "2021-01-30 19:58:47.522",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nSetJobGroup\nBy default a unit of Spark execution is indendent of all others. On a cluster where you have \nmultiple Spark actions or jobs this is makes it difficult to follow the application. \nApplication programmers can use this method to group all of those actions and jobs\ntogether. This gives a group description. Once set, the Spark web UI will associate\nsuch jobs with this group.\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:59:05.772",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eSetJobGroup\n\u003cbr  /\u003eBy default a unit of Spark execution is indendent of all others. On a cluster where you have\n\u003cbr  /\u003emultiple Spark actions or jobs this is makes it difficult to follow the application.\n\u003cbr  /\u003eApplication programmers can use this method to group all of those actions and jobs\n\u003cbr  /\u003etogether. This gives a group description. Once set, the Spark web UI will associate\n\u003cbr  /\u003esuch jobs with this group.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727522_366543360",
      "id": "20210122-174230_917226790",
      "dateCreated": "2021-01-30 19:58:47.522",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Retrieve the Spark Context",
      "text": "%pyspark\n\nsc \u003d spark.sparkContext",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.523",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cfont color\u003d\"red\"\u003ePrevious livy session is expired, new livy session is created. Paragraphs that depend on this paragraph need to be re-executed!\u003c/font\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727523_1635443311",
      "id": "20200830-062453_1327233055",
      "dateCreated": "2021-01-30 19:58:47.523",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nspark",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.523",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cfont color\u003d\"red\"\u003ePrevious livy session is expired, new livy session is created. Paragraphs that depend on this paragraph need to be re-executed!\u003c/font\u003e"
          },
          {
            "type": "TEXT",
            "data": "\u003cpyspark.sql.session.SparkSession object at 0x7fd157873050\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727523_749287265",
      "id": "20210121-115217_395565636",
      "dateCreated": "2021-01-30 19:58:47.523",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nIn a Zeppelin notebook the Spark shell is started by the Livy interpreter. You will not start the Spark shell manually.\nFrom a command line you will have to start the Spark shell with either `spark-shell` or `pyspark`.\n\nSpark creates a SparkSession object called spark. The command `spark` will display information about the spark object. \nThis is displaying the memory address.\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.523",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eIn a Zeppelin notebook the Spark shell is started by the Livy interpreter. You will not start the Spark shell manually.\n\u003cbr  /\u003eFrom a command line you will have to start the Spark shell with either \u003ccode\u003espark-shell\u003c/code\u003e or \u003ccode\u003epyspark\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eSpark creates a SparkSession object called spark. The command \u003ccode\u003espark\u003c/code\u003e will display information about the spark object.\n\u003cbr  /\u003eThis is displaying the memory address.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727523_-19871255",
      "id": "20210121-115307_591923780",
      "dateCreated": "2021-01-30 19:58:47.523",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n# Lab\n---",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.524",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch1\u003eLab\u003c/h1\u003e\n\u003chr /\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727524_1434042061",
      "id": "20181114-164844_1661453681",
      "dateCreated": "2021-01-30 19:58:47.524",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Overview the Apache Spark Documentation\n\n1. View the Spark documentation in your web browser by visiting [http://spark.apache.org/docs/2.4.0/](http://spark.apache.org/docs/2.4.0/).\n2. From the Programming Guides menu, select SQL, DataFrames, and Datasets. Briefly review the guide and bookmark the page for later reference.\n3. From the API Docs menu, select either Scala or Python, depending on your language preference. Bookmark the API page for use during class. Later exercises will refer to this documentation.\n4. If you are viewing the Scala API, notice that the package names are displayed on the left. Use the search box or scroll down to find the org.apache.spark.sql package. This package contains most of the classes and objects you will be working with in this course. In particular, note the Dataset class. Although this exercise focuses on DataFrames, remember that DataFrames are simply an alias for Datasets of Row objects. So all the DataFrame operations you will practice using in this exercise are documented on the Dataset class.\n5. If you are viewing the Python API, locate the pyspark.sql module. This module contains most of the classes you will be working with in this course. At the top are some of the key classes in the module. View the API for the DataFrame class; these are the operations you will practice using in this exercise.",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.525",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eOverview the Apache Spark Documentation\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eView the Spark documentation in your web browser by visiting \u003ca href\u003d\"http://spark.apache.org/docs/2.4.0/\"\u003ehttp://spark.apache.org/docs/2.4.0/\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eFrom the Programming Guides menu, select SQL, DataFrames, and Datasets. Briefly review the guide and bookmark the page for later reference.\u003c/li\u003e\n\u003cli\u003eFrom the API Docs menu, select either Scala or Python, depending on your language preference. Bookmark the API page for use during class. Later exercises will refer to this documentation.\u003c/li\u003e\n\u003cli\u003eIf you are viewing the Scala API, notice that the package names are displayed on the left. Use the search box or scroll down to find the org.apache.spark.sql package. This package contains most of the classes and objects you will be working with in this course. In particular, note the Dataset class. Although this exercise focuses on DataFrames, remember that DataFrames are simply an alias for Datasets of Row objects. So all the DataFrame operations you will practice using in this exercise are documented on the Dataset class.\u003c/li\u003e\n\u003cli\u003eIf you are viewing the Python API, locate the pyspark.sql module. This module contains most of the classes you will be working with in this course. At the top are some of the key classes in the module. View the API for the DataFrame class; these are the operations you will practice using in this exercise.\u003c/li\u003e\n\u003c/ol\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727524_1688556287",
      "id": "20171105-200519_752831754",
      "dateCreated": "2021-01-30 19:58:47.524",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Read and Display a JSON File",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.527",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eRead and Display a JSON File\u003c/h3\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727525_-1448154897",
      "id": "20200111-122228_1360988975",
      "dateCreated": "2021-01-30 19:58:47.525",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "1 - Review the devices.json file",
      "text": "%sh\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.527",
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionSupport": false,
          "completionKey": "TAB"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1612036727527_1639269341",
      "id": "20171105-200623_656362182",
      "dateCreated": "2021-01-30 19:58:47.527",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "2 - Upload the data file to the /devsh_loudacre directory in HDFS",
      "text": "%sh",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.528",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1612036727528_1179314014",
      "id": "20171105-201709_849284875",
      "dateCreated": "2021-01-30 19:58:47.528",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "3 - Create a new DataFrame based on the devices.json file in HDFS",
      "text": "%pyspark",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.528",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1612036727528_655683568",
      "id": "20171105-201449_1118165660",
      "dateCreated": "2021-01-30 19:58:47.528",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nSpark has not yet read the data in the file, but it has scanned the file to infer the schema. View the schema, and note that the column names match the record field names in the JSON file.",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.528",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eSpark has not yet read the data in the file, but it has scanned the file to infer the schema. View the schema, and note that the column names match the record field names in the JSON file.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727528_275431879",
      "id": "20200111-135850_551827200",
      "dateCreated": "2021-01-30 19:58:47.528",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "4 - Print the schema of the new dataframe",
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.529",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1612036727529_1218994302",
      "id": "20200111-135914_1639147600",
      "dateCreated": "2021-01-30 19:58:47.529",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nDisplay the data in the DataFrame using the show function. If you don’t pass an argument to show, Spark will display the first 20 rows in the DataFrame. For this step, display the first five rows. Note that the data is displayed in tabular form, using the column names defined in the schema.",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.530",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eDisplay the data in the DataFrame using the show function. If you don’t pass an argument to show, Spark will display the first 20 rows in the DataFrame. For this step, display the first five rows. Note that the data is displayed in tabular form, using the column names defined in the schema.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727530_-567709688",
      "id": "20200111-140906_1219001174",
      "dateCreated": "2021-01-30 19:58:47.530",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "5 - Display the data of the new dataframe",
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.530",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1612036727530_364552858",
      "id": "20200111-140737_1335490660",
      "dateCreated": "2021-01-30 19:58:47.530",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nThe show and printSchema operations are actions—that is, they return a value from the distributed DataFrame to the Spark driver. Both functions display the data in a nicely formatted table. These functions are intended for interactive use in the shell, but do not allow you actually work with the data that is returned. Try using the take action instead, which returns an array (Scala) or list (Python) of Row objects. You can display the data by iterating through the collection.",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.530",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eThe show and printSchema operations are actions—that is, they return a value from the distributed DataFrame to the Spark driver. Both functions display the data in a nicely formatted table. These functions are intended for interactive use in the shell, but do not allow you actually work with the data that is returned. Try using the take action instead, which returns an array (Scala) or list (Python) of Row objects. You can display the data by iterating through the collection.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727530_-1748607874",
      "id": "20200111-141212_598289647",
      "dateCreated": "2021-01-30 19:58:47.530",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "6 - Display the data using an iteration",
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.531",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1612036727531_1469327086",
      "id": "20200111-141240_53229344",
      "dateCreated": "2021-01-30 19:58:47.531",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Query a Dataframe",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.532",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eQuery a Dataframe\u003c/h3\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727531_-1801148315",
      "id": "20200111-163610_1311973668",
      "dateCreated": "2021-01-30 19:58:47.532",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "7 - Count the rows of the dataframe",
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.532",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1612036727532_-1729662049",
      "id": "20200111-163656_483648092",
      "dateCreated": "2021-01-30 19:58:47.532",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nDataFrame transformations typically return another DataFrame. Try using a select transformation to return a DataFrame with only the make and model columns, then display its schema. Note that only the selected columns are in the schema.",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.532",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eDataFrame transformations typically return another DataFrame. Try using a select transformation to return a DataFrame with only the make and model columns, then display its schema. Note that only the selected columns are in the schema.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727532_-262400365",
      "id": "20200111-163939_417159798",
      "dateCreated": "2021-01-30 19:58:47.532",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "8 - Create a dataframe with only the make and model columns",
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.532",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1612036727532_601326651",
      "id": "20200111-164018_1558528758",
      "dateCreated": "2021-01-30 19:58:47.532",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nA query is a series of one or more transformations followed by an action. Spark does not execute the query until you call the action operation. Display the first rows of the new dataframe.",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.532",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eA query is a series of one or more transformations followed by an action. Spark does not execute the query until you call the action operation. Display the first rows of the new dataframe.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727532_1173069707",
      "id": "20200111-164534_621394130",
      "dateCreated": "2021-01-30 19:58:47.532",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "9 - View the content of this new dataframe",
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.532",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1612036727532_1323710077",
      "id": "20200111-164725_1216595747",
      "dateCreated": "2021-01-30 19:58:47.532",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nTransformations in a query can be chained together. Execute a single command to show the results of a query using select and where. The resulting DataFrame will contain only the columns devnum, make, and model, and only the rows where the make is Ronin.",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.532",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eTransformations in a query can be chained together. Execute a single command to show the results of a query using select and where. The resulting DataFrame will contain only the columns devnum, make, and model, and only the rows where the make is Ronin.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727532_1125854621",
      "id": "20200111-165124_839388669",
      "dateCreated": "2021-01-30 19:58:47.532",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "10 - Chain several transformations and an action",
      "text": "%pyspark\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.533",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1612036727533_-1055568790",
      "id": "20200111-165149_141561983",
      "dateCreated": "2021-01-30 19:58:47.533",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n# Result\n**You have now:** explored a single dataframe using simple transformations.\n\n---",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.533",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch1\u003eResult\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eYou have now:\u003c/strong\u003e explored a single dataframe using simple transformations.\u003c/p\u003e\n\u003chr /\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727533_-1880534578",
      "id": "20181119-142716_792318228",
      "dateCreated": "2021-01-30 19:58:47.533",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n# Solution\n---",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.533",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch1\u003eSolution\u003c/h1\u003e\n\u003chr /\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727533_297825209",
      "id": "20171113-155535_1769142099",
      "dateCreated": "2021-01-30 19:58:47.533",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### View the Spark Documentation\n1. View the Spark documentation in your web browser by visiting [http://spark.apache.org/docs/2.4.0/](http://spark.apache.org/docs/2.4.0/).\n2. From the Programming Guides menu, select SQL, DataFrames, and Datasets. Briefly review the guide and bookmark the page for later reference.\n3. From the API Docs menu, select either Scala or Python, depending on your language preference. Bookmark the API page for use during class. Later exercises will refer to this documentation.\n4. If you are viewing the Scala API, notice that the package names are displayed on the left. Use the search box or scroll down to find the org.apache.spark.sql package. This package contains most of the classes and objects you will be working with in this course. In particular, note the Dataset class. Although this exercise focuses on DataFrames, remember that DataFrames are simply an alias for Datasets of Row objects. So all the DataFrame operations you will practice using in this exercise are documented on the Dataset class.\n5. If you are viewing the Python API, locate the pyspark.sql module. This module contains most of the classes you will be working with in this course. At the top are some of the key classes in the module. View the API for the DataFrame class; these are the operations you will practice using in this exercise.",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.538",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eView the Spark Documentation\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eView the Spark documentation in your web browser by visiting \u003ca href\u003d\"http://spark.apache.org/docs/2.4.0/\"\u003ehttp://spark.apache.org/docs/2.4.0/\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eFrom the Programming Guides menu, select SQL, DataFrames, and Datasets. Briefly review the guide and bookmark the page for later reference.\u003c/li\u003e\n\u003cli\u003eFrom the API Docs menu, select either Scala or Python, depending on your language preference. Bookmark the API page for use during class. Later exercises will refer to this documentation.\u003c/li\u003e\n\u003cli\u003eIf you are viewing the Scala API, notice that the package names are displayed on the left. Use the search box or scroll down to find the org.apache.spark.sql package. This package contains most of the classes and objects you will be working with in this course. In particular, note the Dataset class. Although this exercise focuses on DataFrames, remember that DataFrames are simply an alias for Datasets of Row objects. So all the DataFrame operations you will practice using in this exercise are documented on the Dataset class.\u003c/li\u003e\n\u003cli\u003eIf you are viewing the Python API, locate the pyspark.sql module. This module contains most of the classes you will be working with in this course. At the top are some of the key classes in the module. View the API for the DataFrame class; these are the operations you will practice using in this exercise.\u003c/li\u003e\n\u003c/ol\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727538_-393964284",
      "id": "20210121-114139_413683593",
      "dateCreated": "2021-01-30 19:58:47.538",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Read and Display a JSON File",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.538",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eRead and Display a JSON File\u003c/h3\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727538_-1255103348",
      "id": "20210122-185311_907628106",
      "dateCreated": "2021-01-30 19:58:47.538",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "1 - Review the devices.json file",
      "text": "%sh\n\ncat /home/training/training_materials/devsh/data/devices.json",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.538",
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionSupport": false,
          "completionKey": "TAB"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "fontSize": 9.0,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "{\"devnum\":1,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F00L\",\"dev_type\":\"phone\"}\n{\"devnum\":2,\"release_dt\":\"2010-04-19T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"2100\",\"dev_type\":\"phone\"}\n{\"devnum\":3,\"release_dt\":\"2011-02-18T00:00:00.000-08:00\",\"make\":\"MeeToo\",\"model\":\"3.0\",\"dev_type\":\"phone\"}\n{\"devnum\":4,\"release_dt\":\"2011-09-21T00:00:00.000-07:00\",\"make\":\"MeeToo\",\"model\":\"3.1\",\"dev_type\":\"phone\"}\n{\"devnum\":5,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"1\",\"dev_type\":\"phone\"}\n{\"devnum\":6,\"release_dt\":\"2011-11-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"3\",\"dev_type\":\"phone\"}\n{\"devnum\":7,\"release_dt\":\"2010-05-20T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"2\",\"dev_type\":\"phone\"}\n{\"devnum\":8,\"release_dt\":\"2013-07-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"5\",\"dev_type\":\"phone\"}\n{\"devnum\":9,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"1000\",\"dev_type\":\"phone\"}\n{\"devnum\":10,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"MeeToo\",\"model\":\"1.0\",\"dev_type\":\"phone\"}\n{\"devnum\":11,\"release_dt\":\"2011-02-28T00:00:00.000-08:00\",\"make\":\"Sorrento\",\"model\":\"F21L\",\"dev_type\":\"phone\"}\n{\"devnum\":12,\"release_dt\":\"2012-10-25T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"4\",\"dev_type\":\"phone\"}\n{\"devnum\":13,\"release_dt\":\"2011-11-21T00:00:00.000-08:00\",\"make\":\"Sorrento\",\"model\":\"F23L\",\"dev_type\":\"phone\"}\n{\"devnum\":14,\"release_dt\":\"2010-05-25T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"2200\",\"dev_type\":\"phone\"}\n{\"devnum\":15,\"release_dt\":\"2010-06-20T00:00:00.000-07:00\",\"make\":\"Ronin\",\"model\":\"Novelty Note 1\",\"dev_type\":\"phone\"}\n{\"devnum\":16,\"release_dt\":\"2012-07-21T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"2500\",\"dev_type\":\"phone\"}\n{\"devnum\":17,\"release_dt\":\"2013-04-11T00:00:00.000-07:00\",\"make\":\"Ronin\",\"model\":\"Novelty Note 3\",\"dev_type\":\"phone\"}\n{\"devnum\":18,\"release_dt\":\"2011-10-02T00:00:00.000-07:00\",\"make\":\"Ronin\",\"model\":\"Novelty Note 2\",\"dev_type\":\"phone\"}\n{\"devnum\":19,\"release_dt\":\"2013-07-02T00:00:00.000-07:00\",\"make\":\"Ronin\",\"model\":\"Novelty Note 4\",\"dev_type\":\"phone\"}\n{\"devnum\":20,\"release_dt\":\"2012-07-21T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"3A\",\"dev_type\":\"phone\"}\n{\"devnum\":21,\"release_dt\":\"2011-02-18T00:00:00.000-08:00\",\"make\":\"Titanic\",\"model\":\"2300\",\"dev_type\":\"phone\"}\n{\"devnum\":22,\"release_dt\":\"2012-03-12T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F24L\",\"dev_type\":\"phone\"}\n{\"devnum\":23,\"release_dt\":\"2010-11-01T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F20L\",\"dev_type\":\"phone\"}\n{\"devnum\":24,\"release_dt\":\"2012-10-21T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F32L\",\"dev_type\":\"phone\"}\n{\"devnum\":25,\"release_dt\":\"2011-10-01T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F22L\",\"dev_type\":\"phone\"}\n{\"devnum\":26,\"release_dt\":\"2012-06-21T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F30L\",\"dev_type\":\"phone\"}\n{\"devnum\":27,\"release_dt\":\"2009-10-25T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F10L\",\"dev_type\":\"phone\"}\n{\"devnum\":28,\"release_dt\":\"2012-09-21T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"4000\",\"dev_type\":\"phone\"}\n{\"devnum\":29,\"release_dt\":\"2013-11-01T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F41L\",\"dev_type\":\"phone\"}\n{\"devnum\":30,\"release_dt\":\"2013-08-01T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"DeckChairs\",\"dev_type\":\"phone\"}\n{\"devnum\":31,\"release_dt\":\"2012-08-04T00:00:00.000-07:00\",\"make\":\"MeeToo\",\"model\":\"4.1\",\"dev_type\":\"phone\"}\n{\"devnum\":32,\"release_dt\":\"2012-07-21T00:00:00.000-07:00\",\"make\":\"MeeToo\",\"model\":\"4.0\",\"dev_type\":\"phone\"}\n{\"devnum\":33,\"release_dt\":\"2010-05-15T00:00:00.000-07:00\",\"make\":\"MeeToo\",\"model\":\"2.0\",\"dev_type\":\"phone\"}\n{\"devnum\":34,\"release_dt\":\"2009-12-21T00:00:00.000-08:00\",\"make\":\"Titanic\",\"model\":\"2000\",\"dev_type\":\"phone\"}\n{\"devnum\":35,\"release_dt\":\"2012-09-21T00:00:00.000-07:00\",\"make\":\"MeeToo\",\"model\":\"5.0\",\"dev_type\":\"phone\"}\n{\"devnum\":36,\"release_dt\":\"2013-08-01T00:00:00.000-07:00\",\"make\":\"MeeToo\",\"model\":\"5.1\",\"dev_type\":\"phone\"}\n{\"devnum\":37,\"release_dt\":\"2012-08-04T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"3000\",\"dev_type\":\"phone\"}\n{\"devnum\":38,\"release_dt\":\"2008-11-25T00:00:00.000-08:00\",\"make\":\"Titanic\",\"model\":\"1100\",\"dev_type\":\"phone\"}\n{\"devnum\":39,\"release_dt\":\"2013-03-11T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F33L\",\"dev_type\":\"phone\"}\n{\"devnum\":40,\"release_dt\":\"2013-02-11T00:00:00.000-08:00\",\"make\":\"iFruit\",\"model\":\"4A\",\"dev_type\":\"phone\"}\n{\"devnum\":41,\"release_dt\":\"2012-07-04T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F31L\",\"dev_type\":\"phone\"}\n{\"devnum\":42,\"release_dt\":\"2013-06-26T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F40L\",\"dev_type\":\"phone\"}\n{\"devnum\":43,\"release_dt\":\"2009-03-11T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F01L\",\"dev_type\":\"phone\"}\n{\"devnum\":44,\"release_dt\":\"2010-04-10T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F11L\",\"dev_type\":\"phone\"}\n{\"devnum\":45,\"release_dt\":\"2011-09-21T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"2400\",\"dev_type\":\"phone\"}\n{\"devnum\":46,\"release_dt\":\"2013-10-02T00:00:00.000-07:00\",\"make\":\"Ronin\",\"model\":\"S4\",\"dev_type\":\"phone\"}\n{\"devnum\":47,\"release_dt\":\"2010-05-20T00:00:00.000-07:00\",\"make\":\"Ronin\",\"model\":\"S1\",\"dev_type\":\"phone\"}\n{\"devnum\":48,\"release_dt\":\"2013-02-11T00:00:00.000-08:00\",\"make\":\"Ronin\",\"model\":\"S3\",\"dev_type\":\"phone\"}\n{\"devnum\":49,\"release_dt\":\"2012-07-21T00:00:00.000-07:00\",\"make\":\"Ronin\",\"model\":\"S2\",\"dev_type\":\"phone\"}\n{\"devnum\":50,\"release_dt\":\"2013-10-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"5A\",\"dev_type\":\"phone\"}\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727538_1848287991",
      "id": "20200111-123005_1525915571",
      "dateCreated": "2021-01-30 19:58:47.538",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "2 - Upload the data file to the /devsh_loudacre directory in HDFS",
      "text": "%sh\nhdfs dfs -rm -r -skipTrash /devsh_loudacre/devices.json\nhdfs dfs -put /home/training/training_materials/devsh/data/devices.json /devsh_loudacre/\nhdfs dfs -ls /devsh_loudacre",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.538",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionSupport": false,
          "completionKey": "TAB"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "editorHide": false,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "rm: `/devsh_loudacre/devices.json\u0027: No such file or directory\nFound 2 items\n-rw-r--r--   3 zeppelin supergroup       5483 2021-01-22 17:51 /devsh_loudacre/devices.json\ndrwxr-xr-x   - zeppelin supergroup          0 2021-01-22 17:29 /devsh_loudacre/kb\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727538_1899492279",
      "id": "20200111-123604_463190353",
      "dateCreated": "2021-01-30 19:58:47.538",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "3 - Create a new DataFrame based on the devices.json file in HDFS",
      "text": "%pyspark\n\nsc.setJobGroup(\"Exploring DataFrames\",\"Read the devices.json file in a DataFrame\")\ndevDF \u003d spark.read.json(\"/devsh_loudacre/devices.json\")",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.539",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1612036727539_867774815",
      "id": "20200111-124018_1365794443",
      "dateCreated": "2021-01-30 19:58:47.539",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nUse a value/variable, devDF, to capture a stanza of code. This value may be chained together in any subsequent line.\n\nDataFrame uses the .read function to read data. A power of Spark is the large number of datatypes it can read.\nCommon data types include: .avro, .csv, .json, and .text.\n\nNotew: You can use command completion to see Spark session functions. \nType `spark.` followed by the `tab` key to list functions.\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.539",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eUse a value/variable, devDF, to capture a stanza of code. This value may be chained together in any subsequent line.\u003c/p\u003e\n\u003cp\u003eDataFrame uses the .read function to read data. A power of Spark is the large number of datatypes it can read.\n\u003cbr  /\u003eCommon data types include: .avro, .csv, .json, and .text.\u003c/p\u003e\n\u003cp\u003eNotew: You can use command completion to see Spark session functions.\n\u003cbr  /\u003eType \u003ccode\u003espark.\u003c/code\u003e followed by the \u003ccode\u003etab\u003c/code\u003e key to list functions.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727539_1693397605",
      "id": "20210121-120038_1278676328",
      "dateCreated": "2021-01-30 19:58:47.539",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "4 - Print the schema of the new dataframe",
      "text": "%pyspark\n\ndevDF.printSchema()",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.540",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- dev_type: string (nullable \u003d true)\n |-- devnum: long (nullable \u003d true)\n |-- make: string (nullable \u003d true)\n |-- model: string (nullable \u003d true)\n |-- release_dt: string (nullable \u003d true)"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727539_-257806806",
      "id": "20200111-140004_1737621071",
      "dateCreated": "2021-01-30 19:58:47.539",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nSpark uses a construct of lazy program, meaning data is not manipulated until there is a call for an action.\nIn this case Spark has not yet read the data in the file, but it has scanned the file to infer the schema. \nView the schema, and note that the column names match the record field names in the JSON file.",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.540",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eSpark uses a construct of lazy program, meaning data is not manipulated until there is a call for an action.\n\u003cbr  /\u003eIn this case Spark has not yet read the data in the file, but it has scanned the file to infer the schema.\n\u003cbr  /\u003eView the schema, and note that the column names match the record field names in the JSON file.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727540_-1476591948",
      "id": "20210121-114335_1193512831",
      "dateCreated": "2021-01-30 19:58:47.540",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "5 - Display the data of the new dataframe",
      "text": "%pyspark\n\nsc.setJobGroup(\"Exploring DataFrames\",\"Show 5 devices\")\ndevDF.show(5)",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.540",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------+------+--------+-----+--------------------+\n|dev_type|devnum|    make|model|          release_dt|\n+--------+------+--------+-----+--------------------+\n|   phone|     1|Sorrento| F00L|2008-10-21T00:00:...|\n|   phone|     2| Titanic| 2100|2010-04-19T00:00:...|\n|   phone|     3|  MeeToo|  3.0|2011-02-18T00:00:...|\n|   phone|     4|  MeeToo|  3.1|2011-09-21T00:00:...|\n|   phone|     5|  iFruit|    1|2008-10-21T00:00:...|\n+--------+------+--------+-----+--------------------+\nonly showing top 5 rows"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727540_-1575947571",
      "id": "20200111-140953_499550289",
      "dateCreated": "2021-01-30 19:58:47.540",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nThe `show` function is an action. It will manipulate data. \nDisplay the data in the DataFrame using the `show` function. If you don’t pass an argument to `show`, \nSpark will display the first 20 rows in the DataFrame. For this step, display the first five rows. \nNote that the data is displayed in tabular form, using the column names defined in the schema. This\nis the DataFrame.\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.540",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eThe \u003ccode\u003eshow\u003c/code\u003e function is an action. It will manipulate data.\n\u003cbr  /\u003eDisplay the data in the DataFrame using the \u003ccode\u003eshow\u003c/code\u003e function. If you don’t pass an argument to \u003ccode\u003eshow\u003c/code\u003e,\n\u003cbr  /\u003eSpark will display the first 20 rows in the DataFrame. For this step, display the first five rows.\n\u003cbr  /\u003eNote that the data is displayed in tabular form, using the column names defined in the schema. This\n\u003cbr  /\u003eis the DataFrame.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727540_-1577294109",
      "id": "20210121-114432_19150827",
      "dateCreated": "2021-01-30 19:58:47.540",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "6- Display the data using an iteration",
      "text": "%pyspark\n\nsc.setJobGroup(\"Exploring DataFrames\",\"Take 5 devices the print\")\nrows \u003d devDF.take(5)\nfor row in rows: print(row)",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.541",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Row(dev_type\u003du\u0027phone\u0027, devnum\u003d1, make\u003du\u0027Sorrento\u0027, model\u003du\u0027F00L\u0027, release_dt\u003du\u00272008-10-21T00:00:00.000-07:00\u0027)\nRow(dev_type\u003du\u0027phone\u0027, devnum\u003d2, make\u003du\u0027Titanic\u0027, model\u003du\u00272100\u0027, release_dt\u003du\u00272010-04-19T00:00:00.000-07:00\u0027)\nRow(dev_type\u003du\u0027phone\u0027, devnum\u003d3, make\u003du\u0027MeeToo\u0027, model\u003du\u00273.0\u0027, release_dt\u003du\u00272011-02-18T00:00:00.000-08:00\u0027)\nRow(dev_type\u003du\u0027phone\u0027, devnum\u003d4, make\u003du\u0027MeeToo\u0027, model\u003du\u00273.1\u0027, release_dt\u003du\u00272011-09-21T00:00:00.000-07:00\u0027)\nRow(dev_type\u003du\u0027phone\u0027, devnum\u003d5, make\u003du\u0027iFruit\u0027, model\u003du\u00271\u0027, release_dt\u003du\u00272008-10-21T00:00:00.000-07:00\u0027)"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727541_-367899499",
      "id": "20200111-141357_1484033904",
      "dateCreated": "2021-01-30 19:58:47.541",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nThe `show` and `printSchema` operations are actions—that is, they return a value from the distributed DataFrame \nto the Spark driver. Both functions display the data in a nicely formatted table. These functions are intended \nfor interactive use in the shell, but do not allow you to actually work with the data that is returned. \n\nTo work with the data try using the action `take` function instead. The `take` function returns an array (Scala) \nor list (Python) of Row objects. Use Python foreach loop syntax to display the data by iterating through the collection.\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.541",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eThe \u003ccode\u003eshow\u003c/code\u003e and \u003ccode\u003eprintSchema\u003c/code\u003e operations are actions—that is, they return a value from the distributed DataFrame\n\u003cbr  /\u003eto the Spark driver. Both functions display the data in a nicely formatted table. These functions are intended\n\u003cbr  /\u003efor interactive use in the shell, but do not allow you to actually work with the data that is returned.\u003c/p\u003e\n\u003cp\u003eTo work with the data try using the action \u003ccode\u003etake\u003c/code\u003e function instead. The \u003ccode\u003etake\u003c/code\u003e function returns an array (Scala)\n\u003cbr  /\u003eor list (Python) of Row objects. Use Python foreach loop syntax to display the data by iterating through the collection.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727541_-1381563312",
      "id": "20210121-114544_1598172060",
      "dateCreated": "2021-01-30 19:58:47.541",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nThe output is the actual raw record from the dataset, identified as a Row. Following JSON file format \neach field is assigned a field name. The \"u\" identifes the data as UTF formated text. This is coming\nfrom Python not Spark.",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.541",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eThe output is the actual raw record from the dataset, identified as a Row. Following JSON file format\n\u003cbr  /\u003eeach field is assigned a field name. The \u0026ldquo;u\u0026rdquo; identifes the data as UTF formated text. This is coming\n\u003cbr  /\u003efrom Python not Spark.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727541_-1903309369",
      "id": "20210122-175604_52005625",
      "dateCreated": "2021-01-30 19:58:47.541",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Query a DataFrame",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.542",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eQuery a DataFrame\u003c/h3\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727541_-216166127",
      "id": "20210122-185435_6388575",
      "dateCreated": "2021-01-30 19:58:47.541",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "7 - Count the rows of the dataframe",
      "text": "%pyspark\n\nsc.setJobGroup(\"Exploring DataFrames\",\"Count the devices\")\ndevDF.count()",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.542",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "50"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727542_1307081211",
      "id": "20200111-163831_1875186973",
      "dateCreated": "2021-01-30 19:58:47.542",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nThe `count` function returns the number of items in the DataFrame.\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.542",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eThe \u003ccode\u003ecount\u003c/code\u003e function returns the number of items in the DataFrame.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727542_1987615380",
      "id": "20210121-121441_463017055",
      "dateCreated": "2021-01-30 19:58:47.542",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "8 - Create a dataframe with only the make and model columns",
      "text": "%pyspark\n\nmakeModelDF \u003d devDF.select(\"make\",\"model\")\nmakeModelDF.printSchema()",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.542",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- make: string (nullable \u003d true)\n |-- model: string (nullable \u003d true)"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727542_324128674",
      "id": "20200111-164146_2093325511",
      "dateCreated": "2021-01-30 19:58:47.542",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nDataFrame transformations typically return another DataFrame. Try using a `select` transformation \nto return a DataFrame with only the `make` and `model` columns, then display its schema. Note that \nonly the selected columns are in the schema.\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.546",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eDataFrame transformations typically return another DataFrame. Try using a \u003ccode\u003eselect\u003c/code\u003e transformation\n\u003cbr  /\u003eto return a DataFrame with only the \u003ccode\u003emake\u003c/code\u003e and \u003ccode\u003emodel\u003c/code\u003e columns, then display its schema. Note that\n\u003cbr  /\u003eonly the selected columns are in the schema.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727546_1311627290",
      "id": "20210121-114639_1791679204",
      "dateCreated": "2021-01-30 19:58:47.546",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "9 - View the content of this new dataframe",
      "text": "%pyspark\n\nsc.setJobGroup(\"Exploring DataFrames\",\"Show only the make and model\")\nmakeModelDF.show()",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.547",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "editorHide": false,
        "title": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "make": "string",
                      "model": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------+--------------+\n|    make|         model|\n+--------+--------------+\n|Sorrento|          F00L|\n| Titanic|          2100|\n|  MeeToo|           3.0|\n|  MeeToo|           3.1|\n|  iFruit|             1|\n|  iFruit|             3|\n|  iFruit|             2|\n|  iFruit|             5|\n| Titanic|          1000|\n|  MeeToo|           1.0|\n|Sorrento|          F21L|\n|  iFruit|             4|\n|Sorrento|          F23L|\n| Titanic|          2200|\n|   Ronin|Novelty Note 1|\n| Titanic|          2500|\n|   Ronin|Novelty Note 3|\n|   Ronin|Novelty Note 2|\n|   Ronin|Novelty Note 4|\n|  iFruit|            3A|\n+--------+--------------+\nonly showing top 20 rows"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727546_-1982893153",
      "id": "20200111-164907_1112294231",
      "dateCreated": "2021-01-30 19:58:47.546",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nRemember Spark is lazy. A query is a series of one or more transformations followed by an action. \nSpark does not execute the query until you call the action operation. Display the first rows of the \nnew dataframe. Use the Spark DataFrame `show` function.\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.547",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eRemember Spark is lazy. A query is a series of one or more transformations followed by an action.\n\u003cbr  /\u003eSpark does not execute the query until you call the action operation. Display the first rows of the\n\u003cbr  /\u003enew dataframe. Use the Spark DataFrame \u003ccode\u003eshow\u003c/code\u003e function.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727547_-8469527",
      "id": "20210121-114724_1185884919",
      "dateCreated": "2021-01-30 19:58:47.547",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "10 - Chain several transformations and an action",
      "text": "%pyspark\n\nsc.setJobGroup(\"Exploring DataFrames\",\"Chain the transformations and an action\")\ndevDF.select(\"devnum\",\"make\",\"model\").where(\"make \u003d \u0027Ronin\u0027\").show()",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.548",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "editorHide": false,
        "title": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "devnum": "string",
                      "make": "string",
                      "model": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------+-----+--------------+\n|devnum| make|         model|\n+------+-----+--------------+\n|    15|Ronin|Novelty Note 1|\n|    17|Ronin|Novelty Note 3|\n|    18|Ronin|Novelty Note 2|\n|    19|Ronin|Novelty Note 4|\n|    46|Ronin|            S4|\n|    47|Ronin|            S1|\n|    48|Ronin|            S3|\n|    49|Ronin|            S2|\n+------+-----+--------------+"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727548_431747533",
      "id": "20200111-165319_156372582",
      "dateCreated": "2021-01-30 19:58:47.548",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nThe power of Functional Programing is transformations in a query can be chained together. Execute a single \ncommand to show the results of a query using select and where. The resulting DataFrame will contain only the \ncolumns `devnum`, `make`, and `model`, and only the rows where the make is `Ronin`.\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.549",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eThe power of Functional Programing is transformations in a query can be chained together. Execute a single\n\u003cbr  /\u003ecommand to show the results of a query using select and where. The resulting DataFrame will contain only the\n\u003cbr  /\u003ecolumns \u003ccode\u003edevnum\u003c/code\u003e, \u003ccode\u003emake\u003c/code\u003e, and \u003ccode\u003emodel\u003c/code\u003e, and only the rows where the make is \u003ccode\u003eRonin\u003c/code\u003e.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727548_416058335",
      "id": "20210121-114813_1666360635",
      "dateCreated": "2021-01-30 19:58:47.548",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nReview and bookmark web page for Apache Spark Data Types\nReview and bookmark web page for Apache Spark Data Sources",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.550",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eReview and bookmark web page for Apache Spark Data Types\n\u003cbr  /\u003eReview and bookmark web page for Apache Spark Data Sources\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727550_1005466390",
      "id": "20210122-181034_1732174736",
      "dateCreated": "2021-01-30 19:58:47.550",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n# Tear Down\n---",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.551",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch1\u003eTear Down\u003c/h1\u003e\n\u003chr /\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727550_-691853051",
      "id": "20200830-065213_1837423457",
      "dateCreated": "2021-01-30 19:58:47.551",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Delete the Livy session",
      "text": "%sh\n\nsessionId\u003d$(curl -s localhost:8998/sessions | jq \u0027.sessions[0].id\u0027)\ncurl -s localhost:8998/sessions/$sessionId -X DELETE",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.551",
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "{\"msg\":\"deleted\"}"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727551_-1675884508",
      "id": "20200830-065153_912358573",
      "dateCreated": "2021-01-30 19:58:47.551",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Additional resources",
      "text": "%md\nWe hope you\u0027ve enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Cloudera Tutorials](http://cloudera.com/tutorials.html) are your natural next step where you can explore Spark in more depth.\n2. [Cloudera Community](https://community.cloudera.com) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Apache Spark Documentation](https://spark.apache.org/documentation.html) - official Spark documentation.\n4. [Apache Zeppelin Project Home Page](https://zeppelin.apache.org) - official Zeppelin web site.",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.551",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 10.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eWe hope you\u0027ve enjoyed this lab. Below are additional resources that you should find useful:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href\u003d\"http://cloudera.com/tutorials.html\"\u003eCloudera Tutorials\u003c/a\u003e are your natural next step where you can explore Spark in more depth.\u003c/li\u003e\n\u003cli\u003e\u003ca href\u003d\"https://community.cloudera.com\"\u003eCloudera Community\u003c/a\u003e is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\u003c/li\u003e\n\u003cli\u003e\u003ca href\u003d\"https://spark.apache.org/documentation.html\"\u003eApache Spark Documentation\u003c/a\u003e - official Spark documentation.\u003c/li\u003e\n\u003cli\u003e\u003ca href\u003d\"https://zeppelin.apache.org\"\u003eApache Zeppelin Project Home Page\u003c/a\u003e - official Zeppelin web site.\u003c/li\u003e\n\u003c/ol\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1612036727551_123642423",
      "id": "20181116-135131_93712280",
      "dateCreated": "2021-01-30 19:58:47.551",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%angular\n\u003c/br\u003e\n\u003c/br\u003e\n\u003c/br\u003e\n\u003c/br\u003e\n\u003ccenter\u003e\n\u003ca href\u003d\"https://www.cloudera.com/about/training/courses.html\"\u003e\n  \u003cimg src\u003d\"https://www.cloudera.com/content/dam/www/marketing/media-kit/logo-assets/cloudera_logo_darkorange.png\" alt\u003d\"Cloudera Educational Services\" style\u003d\"width:280px;height:40px;border:0;\" align\u003d\"middle\"\u003e\n\u003c/a\u003e\n\u003c/center\u003e\n\u003c/br\u003e\n\u003c/br\u003e",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.552",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 2.0,
        "editorMode": "ace/mode/undefined",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1612036727551_-1369929175",
      "id": "20200110-154537_1406191376",
      "dateCreated": "2021-01-30 19:58:47.551",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%angular\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-30 19:58:47.552",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/undefined",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1612036727552_-871874791",
      "id": "20200110-162013_302547143",
      "dateCreated": "2021-01-30 19:58:47.552",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "/DevSH/Labs/Pyspark/ExploringDataFrames",
  "id": "2FWE4DHP1",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "sh:shared_process": [],
    "jdbc:shared_process": [],
    "spark:shared_process": [],
    "livy:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}